{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: hls4ml in /home/mo623/.local/lib/python3.10/site-packages (0.8.1)\n",
      "Requirement already satisfied: pydigitalwavetools==1.1 in /home/mo623/.local/lib/python3.10/site-packages (from hls4ml) (1.1)\n",
      "Requirement already satisfied: calmjs.parse in /home/mo623/.local/lib/python3.10/site-packages (from hls4ml) (1.3.1)\n",
      "Requirement already satisfied: tabulate in /home/mo623/.local/lib/python3.10/site-packages (from hls4ml) (0.9.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from hls4ml) (5.4.1)\n",
      "Requirement already satisfied: numpy in /home/mo623/.local/lib/python3.10/site-packages (from hls4ml) (1.26.4)\n",
      "Requirement already satisfied: qkeras in /home/mo623/.local/lib/python3.10/site-packages (from hls4ml) (0.9.0)\n",
      "Requirement already satisfied: tensorflow in /home/mo623/.local/lib/python3.10/site-packages (from hls4ml) (2.15.0.post1)\n",
      "Requirement already satisfied: onnx>=1.4.0 in /home/mo623/.local/lib/python3.10/site-packages (from hls4ml) (1.15.0)\n",
      "Requirement already satisfied: h5py in /home/mo623/.local/lib/python3.10/site-packages (from hls4ml) (3.10.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/lib/python3/dist-packages (from onnx>=1.4.0->hls4ml) (4.21.12)\n",
      "Requirement already satisfied: ply>=3.6 in /usr/lib/python3/dist-packages (from calmjs.parse->hls4ml) (3.11)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from calmjs.parse->hls4ml) (59.6.0)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /home/mo623/.local/lib/python3.10/site-packages (from qkeras->hls4ml) (4.66.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23.1 in /usr/lib/python3/dist-packages (from qkeras->hls4ml) (0.23.2)\n",
      "Requirement already satisfied: networkx>=2.1 in /home/mo623/.local/lib/python3.10/site-packages (from qkeras->hls4ml) (3.2.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/lib/python3/dist-packages (from qkeras->hls4ml) (1.8.0)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.2.1 in /home/mo623/.local/lib/python3.10/site-packages (from qkeras->hls4ml) (0.8.0)\n",
      "Requirement already satisfied: keras-tuner>=1.0.1 in /home/mo623/.local/lib/python3.10/site-packages (from qkeras->hls4ml) (1.4.6)\n",
      "Requirement already satisfied: pyparser in /home/mo623/.local/lib/python3.10/site-packages (from qkeras->hls4ml) (1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (2.15.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow->hls4ml) (1.16.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (0.36.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (0.5.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (4.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (23.5.26)\n",
      "Requirement already satisfied: packaging in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (23.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (1.60.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (16.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (2.4.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (1.14.1)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow->hls4ml) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow->hls4ml) (0.37.1)\n",
      "Requirement already satisfied: kt-legacy in /home/mo623/.local/lib/python3.10/site-packages (from keras-tuner>=1.0.1->qkeras->hls4ml) (1.0.5)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from keras-tuner>=1.0.1->qkeras->hls4ml) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/mo623/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->hls4ml) (3.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/mo623/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->hls4ml) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/mo623/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->hls4ml) (3.5.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/mo623/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->hls4ml) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/mo623/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->hls4ml) (1.2.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/mo623/.local/lib/python3.10/site-packages (from tensorflow-model-optimization>=0.2.1->qkeras->hls4ml) (0.1.8)\n",
      "Requirement already satisfied: parse==1.6.5 in /home/mo623/.local/lib/python3.10/site-packages (from pyparser->qkeras->hls4ml) (1.6.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->hls4ml) (0.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mo623/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->hls4ml) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mo623/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->hls4ml) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mo623/.local/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->hls4ml) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mo623/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->hls4ml) (2.1.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->hls4ml) (3.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->hls4ml) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install hls4ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 23:08:55.617027: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-08 23:08:55.617080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-08 23:08:55.618024: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-08 23:08:55.623680: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-08 23:08:56.324585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Unable to import optimizer(s) from expr_templates.py: No module named 'sympy'\n",
      "WARNING: Failed to import handlers from reshape.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from pooling.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from merge.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mo623/.local/lib/python3.10/site-packages/hls4ml/converters/__init__.py:27: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import hls4ml\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import activations\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, Dense, BatchNormalization, MaxPooling2D, Activation, Flatten, AveragePooling2D, MaxPool2D, Concatenate\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.qconvolutional import QConv2D\n",
    "from qkeras.qpooling import QAveragePooling2D\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu, smooth_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_num, y_num), _ = tf.keras.datasets.mnist.load_data()\n",
    "X_num = np.expand_dims(X_num, axis=-1).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 16  # image_size / mask_size\n",
    "\n",
    "def make_numbers(X, y):\n",
    "    \"\"\"\n",
    "    Generates a new data sample by placing random MNIST digits around a 128x128 frame\n",
    "\n",
    "    :param X: New empty YOLO data array\n",
    "    :param y: New empty YOLO labels data array\n",
    "    \"\"\"\n",
    "\n",
    "    for _ in range(3): # place three digits around the image\n",
    "\n",
    "        idx = np.random.randint(len(X_num)) # pick a random digit to insert in frame\n",
    "        number = X_num[idx] # grab digit\n",
    "        kls = y_num[idx]    # grab class\n",
    "\n",
    "        px, py = np.random.randint(0, 100), np.random.randint(0, 100) # generate a random position for the digit\n",
    "\n",
    "        # digit belong which mask position\n",
    "        mx, my = (px+14) // grid_size, (py+14) // grid_size # calculate which mask region the digit will beglong to\n",
    "        channels = y[my][mx]\n",
    "\n",
    "        # prevent duplicates\n",
    "        if channels[0] > 0:\n",
    "            continue\n",
    "\n",
    "        channels[0] = 1.0\n",
    "        channels[1] = px - (mx * grid_size)  # x1\n",
    "        channels[2] = py - (my * grid_size)  # y1\n",
    "        channels[3] = 28.0                   # x2\n",
    "        channels[4] = 28.0                   # y2\n",
    "        channels[5 + kls] = 1.0\n",
    "\n",
    "        # Insert digit to frame\n",
    "        X[py:py+28, px:px+28] += number\n",
    "\n",
    "def make_data(size=64):\n",
    "    \"\"\"\n",
    "    Generates a new dataset for YOLO training\n",
    "\n",
    "    :param size: Number of samples to generate for training, can be larger than MNIST dataset\n",
    "    :return: New dataset and labels\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.zeros((size, 128, 128, 1), dtype=np.float32) # New data sample of 128x128\n",
    "    y = np.zeros((size, 8, 8, 15), dtype=np.float32)    # New data output consists of probability, bounding box, and class\n",
    "\n",
    "    # Generate data\n",
    "    for i in range(size):\n",
    "        make_numbers(X[i], y[i])\n",
    "\n",
    "    X = np.clip(X, 0.0, 1.0)\n",
    "    return X, y\n",
    "\n",
    "def show_predict(X, y, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Display single prediction results\n",
    "\n",
    "    :param X: Data sample input\n",
    "    :param y: Model predictions\n",
    "    :param threshold: Probability threshold\n",
    "    \"\"\"\n",
    "\n",
    "    X = X.copy()\n",
    "    for mx in range(8):\n",
    "        for my in range(8):\n",
    "            channels = y[my][mx]\n",
    "            prob, x1, y1, x2, y2 = channels[:5]\n",
    "\n",
    "            # if prob < threshold we won't show any thing\n",
    "            if prob < threshold:\n",
    "                continue\n",
    "\n",
    "            color = 255\n",
    "            # bounding box\n",
    "            px, py = (mx * grid_size) + x1, (my * grid_size) + y1\n",
    "            cv2.rectangle(X, (int(px), int(py)), (int(px + x2), int(py + y2)), -1, 1)\n",
    "\n",
    "            # label\n",
    "            cv2.rectangle(X, (int(px), int(py - 10)), (int(px + 12), int(py)), -1, -1)\n",
    "            kls = np.argmax(channels[5:])\n",
    "            cv2.putText(X, f'{kls}', (int(px + 2), int(py-2)), cv2.FONT_HERSHEY_PLAIN, 0.7, (0.0, 0.0, 0.0))\n",
    "\n",
    "            print(\"digit: \" + str(np.argmax(channels[5:15])) + \", prob: \" + str(prob) + \", x1: \" + str(int(px)) + \", y1: \" + str(int(py)) + \", x2: \" + str(int(px + x2)) + \", y2: \" + str(int(py + y2)))\n",
    "\n",
    "    plt.imshow(X, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit: 3, prob: 1.0, x1: 18, y1: 66, x2: 46, y2: 94\n",
      "digit: 9, prob: 1.0, x1: 55, y1: 78, x2: 83, y2: 106\n",
      "digit: 0, prob: 1.0, x1: 88, y1: 97, x2: 116, y2: 125\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = make_data(size=1)\n",
    "show_predict(X[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 23:08:57.904177: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "idx_p = [0] # probability\n",
    "idx_bb = [1, 2, 3, 4] # bounding box\n",
    "idx_cls = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14] # class\n",
    "\n",
    "@tf.function\n",
    "def loss_bb(y_true, y_pred):\n",
    "    y_true = tf.gather(y_true, idx_bb, axis=-1)\n",
    "    y_pred = tf.gather(y_pred, idx_bb, axis=-1)\n",
    "\n",
    "    loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    return tf.reduce_mean(loss[loss > 0.0])\n",
    "\n",
    "@tf.function\n",
    "def loss_p(y_true, y_pred):\n",
    "    y_true = tf.gather(y_true, idx_p, axis=-1)\n",
    "    y_pred = tf.gather(y_pred, idx_p, axis=-1)\n",
    "\n",
    "    loss = tf.losses.binary_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_sum(loss)\n",
    "\n",
    "@tf.function\n",
    "def loss_cls(y_true, y_pred):\n",
    "    y_true = tf.gather(y_true, idx_cls, axis=-1)\n",
    "    y_pred = tf.gather(y_pred, idx_cls, axis=-1)\n",
    "\n",
    "    loss = tf.losses.binary_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_sum(loss)\n",
    "\n",
    "@tf.function\n",
    "def loss_func(y_true, y_pred):\n",
    "    return loss_bb(y_true, y_pred) + loss_p(y_true, y_pred) + loss_cls(y_true, y_pred)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baby_YOLO_conv():\n",
    "    \"\"\"\n",
    "    Baby YOLO model architecture\n",
    "\n",
    "    :return: Keras model\n",
    "    \"\"\"\n",
    "\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    x = inputs = keras.Input(shape=(128, 128, 1), name='input_1')\n",
    "\n",
    "    x = QConv2D(16, kernel_size=3, padding='same', kernel_quantizer=quantized_bits(15,3,alpha=1), bias_quantizer=quantized_bits(7,0,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x = QActivation(activation=quantized_relu(16,8))(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = BatchNormalization()(x) # size: 64x64\n",
    "\n",
    "    x = QConv2D(16, kernel_size=3, padding='same', kernel_quantizer=quantized_bits(16,2,alpha=1), bias_quantizer=quantized_bits(7,2,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x = QActivation(activation=quantized_relu(16,8))(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = BatchNormalization()(x)  # size: 64x64\n",
    "\n",
    "    x = QConv2D(16, kernel_size=3, padding='same', kernel_quantizer=quantized_bits(16,3,alpha=1), bias_quantizer=quantized_bits(8,2,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x = QActivation(activation=quantized_relu(16,8))(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = BatchNormalization()(x)  # size: 32x32\n",
    "\n",
    "    x = QConv2D(16, kernel_size=3, padding='same', kernel_quantizer=quantized_bits(16,2,alpha=1), bias_quantizer=quantized_bits(10,2,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x = QActivation(activation=quantized_relu(16,8))(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = BatchNormalization()(x)  # size: 16x16\n",
    "\n",
    "    x_prob = QConv2D(1, kernel_size=3, padding='same', name='x_prob', activation='sigmoid', kernel_quantizer=quantized_bits(11,2,alpha=1), bias_quantizer=quantized_bits(2,1,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x_boxes = QConv2D(4, kernel_size=3, padding='same', name='x_boxes', kernel_quantizer=quantized_bits(17,3,alpha=1), bias_quantizer=quantized_bits(4,3,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x_cls = QConv2D(10, kernel_size=3, padding='same', name='x_cls', activation='sigmoid', kernel_quantizer=quantized_bits(20,2,alpha=1), bias_quantizer=quantized_bits(8,1,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "\n",
    "    gate = tf.where(x_prob > 0.5, tf.ones_like(x_prob), tf.zeros_like(x_prob))\n",
    "    x_boxes = x_boxes * gate\n",
    "    x_cls = x_cls * gate\n",
    "\n",
    "    x = Concatenate()([x_prob, x_boxes])\n",
    "    x = Concatenate()([x, x_cls])\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " q_conv2d (QConv2D)          (None, 128, 128, 16)         160       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " q_activation (QActivation)  (None, 128, 128, 16)         0         ['q_conv2d[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 64, 64, 16)           0         ['q_activation[0][0]']        \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 64, 64, 16)           64        ['max_pooling2d[0][0]']       \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " q_conv2d_1 (QConv2D)        (None, 64, 64, 16)           2320      ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " q_activation_1 (QActivatio  (None, 64, 64, 16)           0         ['q_conv2d_1[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 16)           0         ['q_activation_1[0][0]']      \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 32, 32, 16)           64        ['max_pooling2d_1[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " q_conv2d_2 (QConv2D)        (None, 32, 32, 16)           2320      ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " q_activation_2 (QActivatio  (None, 32, 32, 16)           0         ['q_conv2d_2[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 16)           0         ['q_activation_2[0][0]']      \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 16, 16, 16)           64        ['max_pooling2d_2[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " q_conv2d_3 (QConv2D)        (None, 16, 16, 16)           2320      ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " q_activation_3 (QActivatio  (None, 16, 16, 16)           0         ['q_conv2d_3[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 8, 8, 16)             0         ['q_activation_3[0][0]']      \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 8, 8, 16)             64        ['max_pooling2d_3[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " x_prob (QConv2D)            (None, 8, 8, 1)              145       ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.math.greater (TFOpLambd  (None, 8, 8, 1)              0         ['x_prob[0][0]']              \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.ones_like (TFOpLambda)   (None, 8, 8, 1)              0         ['x_prob[0][0]']              \n",
      "                                                                                                  \n",
      " tf.zeros_like (TFOpLambda)  (None, 8, 8, 1)              0         ['x_prob[0][0]']              \n",
      "                                                                                                  \n",
      " x_boxes (QConv2D)           (None, 8, 8, 4)              580       ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.where (TFOpLambda)       (None, 8, 8, 1)              0         ['tf.math.greater[0][0]',     \n",
      "                                                                     'tf.ones_like[0][0]',        \n",
      "                                                                     'tf.zeros_like[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 8, 8, 4)              0         ['x_boxes[0][0]',             \n",
      " da)                                                                 'tf.where[0][0]']            \n",
      "                                                                                                  \n",
      " x_cls (QConv2D)             (None, 8, 8, 10)             1450      ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 8, 8, 5)              0         ['x_prob[0][0]',              \n",
      "                                                                     'tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLa  (None, 8, 8, 10)             0         ['x_cls[0][0]',               \n",
      " mbda)                                                               'tf.where[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 8, 8, 15)             0         ['concatenate[0][0]',         \n",
      " )                                                                   'tf.math.multiply_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9551 (37.31 KB)\n",
      "Trainable params: 9423 (36.81 KB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = baby_YOLO_conv()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss_func, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "400/400 [==============================] - 35s 81ms/step - loss: 278.8725\n",
      "Epoch 2/30\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 120.5338\n",
      "Epoch 3/30\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 105.7590\n",
      "Epoch 4/30\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 94.7849\n",
      "Epoch 5/30\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 89.7018\n",
      "Epoch 6/30\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 84.4192\n",
      "Epoch 7/30\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 80.2943\n",
      "Epoch 8/30\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 76.6095\n",
      "Epoch 9/30\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 74.3181\n",
      "Epoch 10/30\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 72.7497\n",
      "Epoch 11/30\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 70.2090\n",
      "Epoch 12/30\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 66.6669\n",
      "Epoch 13/30\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 65.6208\n",
      "Epoch 14/30\n",
      "400/400 [==============================] - 32s 81ms/step - loss: 64.7282\n",
      "Epoch 15/30\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 63.2697\n",
      "Epoch 16/30\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 62.6177\n",
      "Epoch 17/30\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 60.5274\n",
      "Epoch 18/30\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 59.6207\n",
      "Epoch 19/30\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 57.6625\n",
      "Epoch 20/30\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 56.8552\n",
      "Epoch 21/30\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 56.4152\n",
      "Epoch 22/30\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 52.8457\n",
      "Epoch 23/30\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 53.8672\n",
      "Epoch 24/30\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 52.4888\n",
      "Epoch 25/30\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 53.1517\n",
      "Epoch 26/30\n",
      "400/400 [==============================] - 34s 86ms/step - loss: 51.4881\n",
      "Epoch 27/30\n",
      "400/400 [==============================] - 34s 86ms/step - loss: 50.3420\n",
      "Epoch 28/30\n",
      "400/400 [==============================] - 34s 86ms/step - loss: 49.5201\n",
      "Epoch 29/30\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 47.8783\n",
      "Epoch 30/30\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 47.2749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8410083280>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "X_train, y_train = make_data(size=batch_size * 400)\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Layer KReduce already registered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemplate\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m     68\u001b[0m \u001b[39m# Register the converter for custom Keras layer\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m hls4ml\u001b[39m.\u001b[39;49mconverters\u001b[39m.\u001b[39;49mregister_keras_layer_handler(\u001b[39m'\u001b[39;49m\u001b[39mKReduce\u001b[39;49m\u001b[39m'\u001b[39;49m, parse_reduce_layer)\n\u001b[1;32m     71\u001b[0m \u001b[39m# Register the hls4ml's IR layer\u001b[39;00m\n\u001b[1;32m     72\u001b[0m hls4ml\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mregister_layer(\u001b[39m'\u001b[39m\u001b[39mHReduce\u001b[39m\u001b[39m'\u001b[39m, HReduce)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hls4ml/converters/keras_to_hls.py:112\u001b[0m, in \u001b[0;36mregister_keras_layer_handler\u001b[0;34m(layer_cname, handler_func)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m\"\"\"Register a handler function for the given layer class name.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[39mThe handler function should have the following signature:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m    Exception: If the layer class has already been registered.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m layer_cname \u001b[39min\u001b[39;00m layer_handlers:\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLayer \u001b[39m\u001b[39m{\u001b[39;00mlayer_cname\u001b[39m}\u001b[39;00m\u001b[39m already registered\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     layer_handlers[layer_cname] \u001b[39m=\u001b[39m handler_func\n",
      "\u001b[0;31mException\u001b[0m: Layer KReduce already registered"
     ]
    }
   ],
   "source": [
    "class KReduce(tf.keras.layers.Layer):\n",
    "    '''Keras implementation of custom layer'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: a tensor of shape(8,8,15)\n",
    "        return inputs[:, :, :, 0] # We want our hardware model output to be of shape (1,8,8,1)\n",
    "\n",
    "class HReduce(hls4ml.model.layers.Layer):\n",
    "    '''hls4ml implementation of custom layer'''\n",
    "\n",
    "    def initialize(self):\n",
    "        inp = self.get_input_variable()\n",
    "        shape = list(inp.shape)\n",
    "        shape[-1] = 1 # new output shape is (1,8,8,1)\n",
    "\n",
    "        dims = ['OUT_HEIGHT_{}'.format(self.index), 'OUT_WIDTH_{}'.format(self.index), 'N_FILT_{}'.format(self.index)]\n",
    "\n",
    "        self.add_output_variable(shape, dims)\n",
    "\n",
    "def parse_reduce_layer(keras_layer, input_names, input_shapes, data_reader):\n",
    "    '''Parser for converter'''\n",
    "\n",
    "    layer = {}\n",
    "    layer['class_name'] = 'HReduce'\n",
    "    #layer['name'] = keras_layer['config']['name']\n",
    "    layer['name'] = 'quantized_relu'\n",
    "    layer['n_in'] = input_shapes[0][1]*input_shapes[0][2]\n",
    "    layer['grid_size'] = grid_size\n",
    "    outshape = [[None, 8, 8]]\n",
    "\n",
    "    if input_names is not None:\n",
    "        layer['inputs'] = input_names\n",
    "\n",
    "    return layer, outshape\n",
    "\n",
    "# HLS model layer configuration\n",
    "red_config_template = \"\"\"struct config{index} : nnet::reduce_config {{\n",
    "    static const unsigned n_in = {n_in};\n",
    "    static const unsigned grid_size = {grid_size};\n",
    "}};\\n\"\"\"\n",
    "\n",
    "red_function_template ='nnet::reduce<{input_t}, {output_t}, {config}>({input}, {output});'\n",
    "red_include_list = ['nnet_utils/nnet_reduce.h']\n",
    "\n",
    "class HReduceConfigTemplate(hls4ml.backends.template.LayerConfigTemplate):\n",
    "    def __init__(self):\n",
    "        super().__init__(HReduce)\n",
    "        self.template = red_config_template\n",
    "\n",
    "    def format(self, node):\n",
    "        params = self._default_config_params(node)\n",
    "        return self.template.format(**params)\n",
    "\n",
    "class HReduceFunctionTemplate(hls4ml.backends.template.FunctionCallTemplate):\n",
    "    '''New layer function template'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(HReduce, include_header=red_include_list)\n",
    "        self.template = red_function_template\n",
    "\n",
    "    def format(self, node):\n",
    "        params = self._default_function_params(node)\n",
    "        return self.template.format(**params)\n",
    "\n",
    "# Register the converter for custom Keras layer\n",
    "hls4ml.converters.register_keras_layer_handler('KReduce', parse_reduce_layer)\n",
    "\n",
    "# Register the hls4ml's IR layer\n",
    "hls4ml.model.layers.register_layer('HReduce', HReduce)\n",
    "\n",
    "for backend_id in ['Vivado']:\n",
    "    # Register the optimization passes (if any)\n",
    "    backend = hls4ml.backends.get_backend(backend_id)\n",
    "\n",
    "    # Register template passes for the given backend\n",
    "    backend.register_template(HReduceConfigTemplate)\n",
    "    backend.register_template(HReduceFunctionTemplate)\n",
    "\n",
    "    # Register HLS implementation\n",
    "    backend.register_source(os.path.abspath(\"/content/drive/MyDrive/nnet_reduce.h\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " q_conv2d (QConv2D)          (None, 128, 128, 16)         160       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " q_activation (QActivation)  (None, 128, 128, 16)         0         ['q_conv2d[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 64, 64, 16)           0         ['q_activation[0][0]']        \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 64, 64, 16)           64        ['max_pooling2d[0][0]']       \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " q_conv2d_1 (QConv2D)        (None, 64, 64, 16)           2320      ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " q_activation_1 (QActivatio  (None, 64, 64, 16)           0         ['q_conv2d_1[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 16)           0         ['q_activation_1[0][0]']      \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 32, 32, 16)           64        ['max_pooling2d_1[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " q_conv2d_2 (QConv2D)        (None, 32, 32, 16)           2320      ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " q_activation_2 (QActivatio  (None, 32, 32, 16)           0         ['q_conv2d_2[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 16)           0         ['q_activation_2[0][0]']      \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 16, 16, 16)           64        ['max_pooling2d_2[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " q_conv2d_3 (QConv2D)        (None, 16, 16, 16)           2320      ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " q_activation_3 (QActivatio  (None, 16, 16, 16)           0         ['q_conv2d_3[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 8, 8, 16)             0         ['q_activation_3[0][0]']      \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 8, 8, 16)             64        ['max_pooling2d_3[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " x_prob (QConv2D)            (None, 8, 8, 1)              145       ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " x_boxes (QConv2D)           (None, 8, 8, 4)              580       ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 8, 8, 5)              0         ['x_prob[0][0]',              \n",
      "                                                                     'x_boxes[0][0]']             \n",
      "                                                                                                  \n",
      " x_cls (QConv2D)             (None, 8, 8, 10)             1450      ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 8, 8, 15)             0         ['concatenate[0][0]',         \n",
      " )                                                                   'x_cls[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9551 (37.31 KB)\n",
      "Trainable params: 9423 (36.81 KB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# class quantized_relu(quantized_relu):\n",
    "#     ''' Workaround for quantized_relu __name__ not found bug '''\n",
    "#     def __init__(self, W, I):\n",
    "#         super(quantized_relu, self).__init__(W, I)\n",
    "#         self.__name__ = 'quantized_relu'\n",
    "\n",
    "def baby_YOLO_conv_reduce():\n",
    "    \"\"\"\n",
    "    Baby YOLO model architecture with new custom layer\n",
    "\n",
    "    :return: Keras model\n",
    "    \"\"\"\n",
    "\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    x = inputs = keras.Input(shape=(128, 128, 1), name='input_1')\n",
    "\n",
    "    x = QConv2D(16, kernel_size=3, padding='same', kernel_quantizer=quantized_bits(15,3,alpha=1), bias_quantizer=quantized_bits(7,0,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x = QActivation(activation=quantized_relu(16,8))(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = BatchNormalization()(x) # size: 64x64\n",
    "\n",
    "    x = QConv2D(16, kernel_size=3, padding='same', kernel_quantizer=quantized_bits(16,2,alpha=1), bias_quantizer=quantized_bits(7,2,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x = QActivation(activation=quantized_relu(16,8))(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = BatchNormalization()(x)  # size: 64x64\n",
    "\n",
    "    x = QConv2D(16, kernel_size=3, padding='same', kernel_quantizer=quantized_bits(16,3,alpha=1), bias_quantizer=quantized_bits(8,2,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x = QActivation(activation=quantized_relu(16,8))(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = BatchNormalization()(x)  # size: 32x32\n",
    "\n",
    "    x = QConv2D(16, kernel_size=3, padding='same', kernel_quantizer=quantized_bits(16,2,alpha=1), bias_quantizer=quantized_bits(10,2,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x = QActivation(activation=quantized_relu(16,8))(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    x = BatchNormalization()(x)  # size: 16x16\n",
    "\n",
    "    x_prob = QConv2D(1, kernel_size=3, padding='same', name='x_prob', activation='sigmoid', kernel_quantizer=quantized_bits(11,2,alpha=1), bias_quantizer=quantized_bits(2,1,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x_boxes = QConv2D(4, kernel_size=3, padding='same', name='x_boxes', kernel_quantizer=quantized_bits(17,3,alpha=1), bias_quantizer=quantized_bits(4,3,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "    x_cls = QConv2D(10, kernel_size=3, padding='same', name='x_cls', activation='sigmoid', kernel_quantizer=quantized_bits(20,2,alpha=1), bias_quantizer=quantized_bits(8,1,alpha=1), kernel_initializer='lecun_uniform')(x)\n",
    "\n",
    "    x = Concatenate()([x_prob, x_boxes])\n",
    "    x = Concatenate()([x, x_cls])\n",
    "\n",
    "    #x = KReduce()(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "model_reduce = baby_YOLO_conv_reduce()\n",
    "model_reduce.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pydot' has no attribute 'InvocationException'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pydot/core.py:1753\u001b[0m, in \u001b[0;36mDot.create\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1753\u001b[0m     stdout_data, stderr_data, process \u001b[39m=\u001b[39m call_graphviz(\n\u001b[1;32m   1754\u001b[0m         program\u001b[39m=\u001b[39;49mprog,\n\u001b[1;32m   1755\u001b[0m         arguments\u001b[39m=\u001b[39;49marguments,\n\u001b[1;32m   1756\u001b[0m         working_dir\u001b[39m=\u001b[39;49mtmp_dir,\n\u001b[1;32m   1757\u001b[0m     )\n\u001b[1;32m   1758\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pydot/core.py:133\u001b[0m, in \u001b[0;36mcall_graphviz\u001b[0;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m program_with_args \u001b[39m=\u001b[39m [program] \u001b[39m+\u001b[39m arguments\n\u001b[0;32m--> 133\u001b[0m process \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mPopen(\n\u001b[1;32m    134\u001b[0m     program_with_args,\n\u001b[1;32m    135\u001b[0m     env\u001b[39m=\u001b[39;49menv,\n\u001b[1;32m    136\u001b[0m     cwd\u001b[39m=\u001b[39;49mworking_dir,\n\u001b[1;32m    137\u001b[0m     shell\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    138\u001b[0m     stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m    139\u001b[0m     stdout\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m    140\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    142\u001b[0m stdout_data, stderr_data \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mcommunicate()\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    972\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    973\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    974\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    975\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    976\u001b[0m                         errread, errwrite,\n\u001b[1;32m    977\u001b[0m                         restore_signals,\n\u001b[1;32m    978\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    979\u001b[0m                         start_new_session)\n\u001b[1;32m    980\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/vis_utils.py:57\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Attempt to create an image of a blank graph\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39m# to check the pydot/graphviz installation.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     pydot\u001b[39m.\u001b[39;49mDot\u001b[39m.\u001b[39;49mcreate(pydot\u001b[39m.\u001b[39;49mDot())\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pydot/core.py:1762\u001b[0m, in \u001b[0;36mDot.create\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1761\u001b[0m     args[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{prog}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m not found in path.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(prog\u001b[39m=\u001b[39mprog)\n\u001b[0;32m-> 1762\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1763\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mplot_model(model_reduce)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/vis_utils.py:451\u001b[0m, in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations, show_trainable)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis model has not yet been built. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe model on a batch of data.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     )\n\u001b[0;32m--> 451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_graphviz():\n\u001b[1;32m    452\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand install graphviz \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(see instructions at https://graphviz.gitlab.io/download/) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor plot_model to work.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m     )\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mIPython.core.magics.namespace\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules:\n\u001b[1;32m    459\u001b[0m         \u001b[39m# We don't raise an exception here in order to avoid crashing\u001b[39;00m\n\u001b[1;32m    460\u001b[0m         \u001b[39m# notebook tests where graphviz is not available.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/vis_utils.py:59\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     pydot\u001b[39m.\u001b[39mDot\u001b[39m.\u001b[39mcreate(pydot\u001b[39m.\u001b[39mDot())\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, pydot\u001b[39m.\u001b[39;49mInvocationException):\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pydot' has no attribute 'InvocationException'"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(d, indent=0):\n",
    "    \"\"\"\n",
    "    Prints dictionary to view hls4ml model config\n",
    "\n",
    "    :return: Keras model\n",
    "    \"\"\"\n",
    "\n",
    "    align = 20\n",
    "    for key, value in d.items():\n",
    "        print('  ' * indent + str(key), end='')\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            print_dict(value, indent+1)\n",
    "        else:\n",
    "            print(':' + ' ' * (20 - len(key) - 2 * indent) + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 128, 128, 1]], output shape: [None, 128, 128, 1]\n",
      "Layer name: q_conv2d, layer type: QConv2D, input shapes: [[None, 128, 128, 1]], output shape: [None, 128, 128, 16]\n",
      "Layer name: q_activation, layer type: Activation, input shapes: [[None, 128, 128, 16]], output shape: [None, 128, 128, 16]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, input shapes: [[None, 128, 128, 16]], output shape: [None, 64, 64, 16]\n",
      "Layer name: batch_normalization, layer type: BatchNormalization, input shapes: [[None, 64, 64, 16]], output shape: [None, 64, 64, 16]\n",
      "Layer name: q_conv2d_1, layer type: QConv2D, input shapes: [[None, 64, 64, 16]], output shape: [None, 64, 64, 16]\n",
      "Layer name: q_activation_1, layer type: Activation, input shapes: [[None, 64, 64, 16]], output shape: [None, 64, 64, 16]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, input shapes: [[None, 64, 64, 16]], output shape: [None, 32, 32, 16]\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization, input shapes: [[None, 32, 32, 16]], output shape: [None, 32, 32, 16]\n",
      "Layer name: q_conv2d_2, layer type: QConv2D, input shapes: [[None, 32, 32, 16]], output shape: [None, 32, 32, 16]\n",
      "Layer name: q_activation_2, layer type: Activation, input shapes: [[None, 32, 32, 16]], output shape: [None, 32, 32, 16]\n",
      "Layer name: max_pooling2d_2, layer type: MaxPooling2D, input shapes: [[None, 32, 32, 16]], output shape: [None, 16, 16, 16]\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization, input shapes: [[None, 16, 16, 16]], output shape: [None, 16, 16, 16]\n",
      "Layer name: q_conv2d_3, layer type: QConv2D, input shapes: [[None, 16, 16, 16]], output shape: [None, 16, 16, 16]\n",
      "Layer name: q_activation_3, layer type: Activation, input shapes: [[None, 16, 16, 16]], output shape: [None, 16, 16, 16]\n",
      "Layer name: max_pooling2d_3, layer type: MaxPooling2D, input shapes: [[None, 16, 16, 16]], output shape: [None, 8, 8, 16]\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization, input shapes: [[None, 8, 8, 16]], output shape: [None, 8, 8, 16]\n",
      "Layer name: x_prob, layer type: QConv2D, input shapes: [[None, 8, 8, 16]], output shape: [None, 8, 8, 1]\n",
      "Layer name: x_boxes, layer type: QConv2D, input shapes: [[None, 8, 8, 16]], output shape: [None, 8, 8, 4]\n",
      "Layer name: concatenate, layer type: Concatenate, input shapes: [[None, 8, 8, 1], [None, 8, 8, 4]], output shape: [None, 8, 8, 5]\n",
      "Layer name: x_cls, layer type: QConv2D, input shapes: [[None, 8, 8, 16]], output shape: [None, 8, 8, 10]\n",
      "Layer name: concatenate_1, layer type: Concatenate, input shapes: [[None, 8, 8, 5], [None, 8, 8, 10]], output shape: [None, 8, 8, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mo623/.local/lib/python3.10/site-packages/keras/src/constraints.py:365: UserWarning: The `keras.constraints.serialize()` API should only be used for objects of type `keras.constraints.Constraint`. Found an instance of type <class 'qkeras.quantizers.quantized_bits'>, which may lead to improper serialization.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'k_reduce'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m config[\u001b[39m\"\u001b[39m\u001b[39mLayerName\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mx_cls\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mPrecision\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39map_fixed<20,2>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m config[\u001b[39m\"\u001b[39m\u001b[39mLayerName\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mx_cls\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mPrecision\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39map_fixed<8,1>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 62\u001b[0m config[\u001b[39m\"\u001b[39;49m\u001b[39mLayerName\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mk_reduce\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39m\u001b[39mPrecision\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39map_uint<33>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m print_dict(config)\n\u001b[1;32m     66\u001b[0m hls_model \u001b[39m=\u001b[39m hls4ml\u001b[39m.\u001b[39mconverters\u001b[39m.\u001b[39mconvert_from_keras_model(model_reduce,\n\u001b[1;32m     67\u001b[0m                                                        hls_config \u001b[39m=\u001b[39m config,\n\u001b[1;32m     68\u001b[0m                                                        io_type \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mio_stream\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     69\u001b[0m                                                        output_dir \u001b[39m=\u001b[39m OUT_DIR,\n\u001b[1;32m     70\u001b[0m                                                        part\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mxcku035-fbva676-2-e\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     71\u001b[0m                                                        )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'k_reduce'"
     ]
    }
   ],
   "source": [
    "config = hls4ml.utils.config_from_keras_model (model_reduce,\n",
    "                                                   default_precision = 'ap_fixed<16,8>',\n",
    "                                                   granularity = 'name')\n",
    "\n",
    "strategy = \"Latency\"\n",
    "rf = 1\n",
    "tracing = True\n",
    "OUT_DIR = f'yolo_on_frame_grabber'\n",
    "\n",
    "config[\"Model\"][\"Trace\"] = tracing\n",
    "\n",
    "config[\"Model\"][\"Strategy\"] = \"Resource\"\n",
    "for layer in config[\"LayerName\"]:\n",
    "    config[\"LayerName\"][layer][\"Trace\"] = tracing\n",
    "    config[\"LayerName\"][layer]['ReuseFactor'] = rf\n",
    "    config[\"LayerName\"][layer][\"Strategy\"] = strategy\n",
    "\n",
    "config[\"LayerName\"][\"q_conv2d\"]['ReuseFactor'] = 1\n",
    "config[\"LayerName\"][\"q_conv2d\"][\"Strategy\"] = \"Latency\"\n",
    "config[\"LayerName\"][\"q_conv2d\"][\"Precision\"][\"weight\"] = \"ap_fixed<15,3>\"\n",
    "config[\"LayerName\"][\"q_conv2d\"][\"Precision\"][\"bias\"] = \"ap_fixed<7,0>\"\n",
    "\n",
    "# config[\"LayerName\"][\"batch_normalization\"][\"Precision\"][\"scale\"] = \"ap_fixed<7,7>\"\n",
    "# config[\"LayerName\"][\"batch_normalization\"][\"Precision\"][\"bias\"] = \"ap_fixed<9,3>\"\n",
    "\n",
    "config[\"LayerName\"][\"q_conv2d_1\"]['ReuseFactor'] = 4\n",
    "config[\"LayerName\"][\"q_conv2d_1\"][\"Strategy\"] = \"Latency\"\n",
    "config[\"LayerName\"][\"q_conv2d_1\"][\"Precision\"][\"weight\"] = \"ap_fixed<16,2>\"\n",
    "config[\"LayerName\"][\"q_conv2d_1\"][\"Precision\"][\"bias\"] = \"ap_fixed<7,2>\"\n",
    "\n",
    "# config[\"LayerName\"][\"batch_normalization_1\"][\"Precision\"][\"scale\"] = \"ap_fixed<6,3>\"\n",
    "# config[\"LayerName\"][\"batch_normalization_1\"][\"Precision\"][\"bias\"] = \"ap_fixed<8,3>\"\n",
    "\n",
    "config[\"LayerName\"][\"q_conv2d_2\"]['ReuseFactor'] = 12\n",
    "config[\"LayerName\"][\"q_conv2d_2\"][\"Strategy\"] = \"Resource\"\n",
    "config[\"LayerName\"][\"q_conv2d_2\"][\"Precision\"][\"weight\"] = \"ap_fixed<16,3>\"\n",
    "config[\"LayerName\"][\"q_conv2d_2\"][\"Precision\"][\"bias\"] = \"ap_fixed<8,2>\"\n",
    "\n",
    "# config[\"LayerName\"][\"batch_normalization_2\"][\"Precision\"][\"scale\"] = \"ap_fixed<4,1>\"\n",
    "# config[\"LayerName\"][\"batch_normalization_2\"][\"Precision\"][\"bias\"] = \"ap_fixed<7,3>\"\n",
    "\n",
    "config[\"LayerName\"][\"q_conv2d_3\"]['ReuseFactor'] = 48\n",
    "config[\"LayerName\"][\"q_conv2d_3\"][\"Strategy\"] = \"Resource\"\n",
    "config[\"LayerName\"][\"q_conv2d_3\"][\"Precision\"][\"weight\"] = \"ap_fixed<16,2>\"\n",
    "config[\"LayerName\"][\"q_conv2d_3\"][\"Precision\"][\"bias\"] = \"ap_fixed<10,2>\"\n",
    "\n",
    "# config[\"LayerName\"][\"batch_normalization_3\"][\"Precision\"][\"scale\"] = \"ap_fixed<5,1>\"\n",
    "# config[\"LayerName\"][\"batch_normalization_3\"][\"Precision\"][\"bias\"] = \"ap_fixed<9,4>\"\n",
    "\n",
    "config[\"LayerName\"][\"x_prob\"]['ReuseFactor'] = 48\n",
    "config[\"LayerName\"][\"x_prob\"][\"Precision\"][\"weight\"] = \"ap_fixed<11,2>\"\n",
    "config[\"LayerName\"][\"x_prob\"][\"Precision\"][\"bias\"] = \"ap_fixed<2,1>\"\n",
    "\n",
    "config[\"LayerName\"][\"x_boxes\"]['ReuseFactor'] = 48\n",
    "config[\"LayerName\"][\"x_boxes\"][\"Precision\"][\"weight\"] = \"ap_fixed<17,3>\"\n",
    "config[\"LayerName\"][\"x_boxes\"][\"Precision\"][\"bias\"] = \"ap_fixed<4,3>\"\n",
    "\n",
    "config[\"LayerName\"][\"x_cls\"]['ReuseFactor'] = 48\n",
    "config[\"LayerName\"][\"x_cls\"][\"Precision\"][\"weight\"] = \"ap_fixed<20,2>\"\n",
    "config[\"LayerName\"][\"x_cls\"][\"Precision\"][\"bias\"] = \"ap_fixed<8,1>\"\n",
    "\n",
    "config[\"LayerName\"][\"k_reduce\"][\"Precision\"][\"result\"] = \"ap_uint<33>\"\n",
    "\n",
    "print_dict(config)\n",
    "\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(model_reduce,\n",
    "                                                       hls_config = config,\n",
    "                                                       io_type = 'io_stream',\n",
    "                                                       output_dir = OUT_DIR,\n",
    "                                                       part='xcku035-fbva676-2-e'\n",
    "                                                       )\n",
    "\n",
    "hls_model.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
